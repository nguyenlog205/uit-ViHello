{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d68d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb011cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.model import TinyGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687fc54",
   "metadata": {},
   "source": [
    "## 1. Executive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280bf160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scientific_summary(model, active_params=None, use_moe=False):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if active_params is None:\n",
    "        active_params = total_params\n",
    "        efficiency = 1.0\n",
    "        active_note = \"(Dense Model)\"\n",
    "    else:\n",
    "        efficiency = total_params / active_params\n",
    "        active_note = \"(Top-1 MoE)\" if use_moe else \"(Sparse)\"\n",
    "\n",
    "    bytes_fp32 = total_params * 4\n",
    "    mb_fp32 = bytes_fp32 / (1024 * 1024)\n",
    "\n",
    "    line_double = \"=\" * 110\n",
    "    line_single = \"-\" * 110\n",
    "    \n",
    "    print(line_double)\n",
    "    print(f\"{'VIHELLO MODEL SCIENTIFIC SUMMARY':^110}\")\n",
    "    print(line_double)\n",
    "    \n",
    "    print(f\"Total Parameters:         {total_params:,.0f}\")\n",
    "    print(f\"Active Parameters:        {active_params:,.1f} {active_note}\")\n",
    "    print(f\"Model Efficiency:         {efficiency:.1f}x (Compute saving)\")\n",
    "    \n",
    "    print(line_single)\n",
    "\n",
    "    print(f\"{'Format':<18} | {'Bit':<5} | {'Size (MB)':<12} | {'Compression':<12} | {'Mem Saving':<12} | {'Note'}\")\n",
    "    print(line_single)\n",
    "    \n",
    "    formats = [\n",
    "        (\"FP32 (Original)\", 32, mb_fp32,       1.0, 0.0,  \"Master Copy\"),\n",
    "        (\"BF16/FP16\",       16, mb_fp32 / 2,   2.0, 50.0, \"Standard\"),\n",
    "        (\"INT8\",            8,  mb_fp32 / 4,   4.0, 75.0, \"Standard\"),\n",
    "        (\"INT4 (GGUF)\",     4,  mb_fp32 / 8,   8.0, 87.5, \"Best for Toys\"),\n",
    "    ]\n",
    "    \n",
    "    for name, bits, size, comp, saving, note in formats:\n",
    "        print(f\"{name:<18} | {bits:<5} | {size:>9.2f} MB | {comp:>10.1f}x | {saving:>10.1f}% | {note}\")\n",
    "        \n",
    "    print(line_single)\n",
    "    \n",
    "    print(\"[*] MLA Latent Compression: 4.0x KV-Cache reduction\")\n",
    "    print(\"[*] Inference Speed Grade: S+\")\n",
    "    print(line_double)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2894cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyGPT(vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0227129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "                                       VIHELLO MODEL SCIENTIFIC SUMMARY                                       \n",
      "==============================================================================================================\n",
      "Total Parameters:         214,912\n",
      "Active Parameters:        214,912.0 (Dense Model)\n",
      "Model Efficiency:         1.0x (Compute saving)\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Format             | Bit   | Size (MB)    | Compression  | Mem Saving   | Note\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "FP32 (Original)    | 32    |      0.82 MB |        1.0x |        0.0% | Master Copy\n",
      "BF16/FP16          | 16    |      0.41 MB |        2.0x |       50.0% | Standard\n",
      "INT8               | 8     |      0.20 MB |        4.0x |       75.0% | Standard\n",
      "INT4 (GGUF)        | 4     |      0.10 MB |        8.0x |       87.5% | Best for Toys\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[*] MLA Latent Compression: 4.0x KV-Cache reduction\n",
      "[*] Inference Speed Grade: S+\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_scientific_summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
