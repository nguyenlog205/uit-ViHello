\chapter{Introduction to Language Model}
\section{The core idea of language modeling}
At its core, a \textbf{Language Model (LM)} is a probabilistic distribution over sequences of tokens. 
The objective of language modeling is to estimate the joint probability distribution $P(\mathbf{w})$ over a sequence of discrete symbols $\mathbf{w} = (w_1, w_2, \dots, w_T)$ drawn from a finite vocabulary $\mathcal{V}$. According to \textit{the chain rule of probability}, any joint distribution can be factorized into a product of conditional probabilities:
$$P(w_1, w_2, \dots, w_T) = \prod_{t=1}^{T} P(w_t \mid w_1, \dots, w_{t-1})$$
In the context of generative modeling, this is referred to as Autoregressive Modeling. The model is tasked with predicting the probability of the next token $w_t$ given the preceding sequence, known as the prefix or context $w_{<t}$.

\begin{graybox}
    Given the vocabulary $\mathcal{V} = \{\text{The, a, cat, dog, sat, mat}\}$ and the sequence $\mathbf{w} = (\text{The, cat, sat})$, the joint probability is calculated as follows:
\end{graybox}

Let $w_1 = \text{The}$, $w_2 = \text{cat}$, and $w_3 = \text{sat}$. Applying the chain rule of probability:
\begin{equation*}
P(w_1, w_2, w_3) = P(w_1) \cdot P(w_2 \mid w_1) \cdot P(w_3 \mid w_1, w_2)
\end{equation*}
Substituting the specific tokens from $\mathcal{V}$:

\begin{equation*}
P(\text{The, cat, sat}) = P(\text{The}) \times P(\text{cat} \mid \text{The}) \times P(\text{sat} \mid \text{The, cat})
\end{equation*}
Each term represents the model's prediction at time step $t$ given the preceding context $w_{<t}$.
Based on the probability distribution presented in table \ref{tab:token_4_dist}, the model does not simply output a list of numbers; it must execute a decoding strategy to select the most appropriate discrete token for the sequence.
The choice between candidates like "on" or "mat" depends on the specific objective of the generation:
\begin{itemize}
    \item[+] \textbf{Greedy Decoding}: The model consistently selects the token with the highest probability:$$\hat{w}_t = \text{argmax}_{v \in \mathcal{V}} P(v \mid w_{<t})$$
    
    In this case, the model would select "on" ($P=0.5012$) to continue the sequence as "The dog sat on..."

    \item[+] \textbf{Stochastic Sampling}: To introduce variance and avoid repetitive "robotic" text, the model may sample from the distribution. 
    
    Strategies like top-k or top-p (Nucleus) sampling allow the model to occasionally choose lower-probability candidates like "mat" if they fall within a certain threshold.
\end{itemize}

\begin{table}[h]
\centering
\caption{Probability distribution for $w_4$ given $w_{<4} = (\text{The, dog, sat})$}
\label{tab:token_4_dist}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Index} & \textbf{Token ($v$)} & \textbf{Logit ($z$)} & \textbf{$P(w_4 = v \mid \text{prefix})$} \\ \midrule
1 & on  & 4.12 & 0.5012 \\
2 & mat & 3.89 & 0.3985 \\
3 & a   & 1.45 & 0.0347 \\
4 & the & 1.18 & 0.0265 \\
5 & cat & 0.82 & 0.0185 \\
6 & dog & 0.54 & 0.0140 \\
7 & sat & -0.12 & 0.0066 \\ \midrule
\textbf{Total} & & & \textbf{1.0000} \\ \bottomrule
\end{tabular}
\end{table}

\section{From logits to softmax}
The raw outputs from the final linear layer of a language model are known as logits ($\mathbf{z}$). These values are unbounded, ranging from $(-\infty, +\infty)$, making them unsuitable for direct probabilistic interpretation. To transform these scores into a valid probability distribution where each value $P_i \in [0, 1]$ and $\sum P = 1$, we apply the Softmax function.

For a vocabulary $\mathcal{V}$ of size $K$, the probability of the $i$-th token is defined as:$$P(w_t = v_i \mid w_{<t}) = \frac{\exp(z_i)}{\sum_{j=1}^{K} \exp(z_j)}$$The exponential function $\exp(\cdot)$ ensures that all resulting values are positive, while the denominator (the partition function) normalizes the vector.

Referring back to the empirical values in Table \ref{tab:token_4_dist}, we can observe how the model amplifies the gap between candidates. Even though the logit for "on" ($4.12$) and "mat" ($3.89$) are numerically close, the exponentiation creates a clear distinction in probability mass.
\begin{graybox}
\textbf{Derivation 1.4: Softmax Calculation}
\medskip

Let us calculate the probability for $v_1 = \text{``on''}$ and $v_2 = \text{``mat''}$ using their respective logits $z_1 = 4.12$ and $z_2 = 3.89$. 

First, we compute the exponentials:
\begin{itemize}
    \item $\exp(4.12) \approx 61.56$
    \item $\exp(3.89) \approx 48.91$
\end{itemize}

Assuming the sum of exponentials for the entire vocabulary $\sum \exp(z_j) \approx 122.82$, the resulting probabilities are:
\begin{equation*}
P(\text{on}) = \frac{61.56}{122.82} \approx 0.5012, \quad P(\text{mat}) = \frac{48.91}{122.82} \approx 0.3982
\end{equation*}

This mapping demonstrates how the Softmax function acts as a "winner-take-all" mechanism, aggressively rewarding higher logits while suppressing lower scores.
\end{graybox}